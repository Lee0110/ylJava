# 数据库八股文

## MySQL数据库

### MySQL

#### 1. 一条SQL查询语句是如何执行的

- 连接器：连接器负责跟客户端建立连接、获取权限、维持和管理连接。
- 查询缓存：之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。如果查询命中缓存，MySQL 不需要执行后面的复杂操作，就可以直接返回结果，这个效率会很高。但是大多时候弊大于利，查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。MySQL8.0版本直接将查询缓存模块删除了。
- 分析器：词法分析，语法分析
- 优化器：优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。
- 执行器：开始执行的时候，要先判断一下你对这个表有没有执行查询的权限，如果没有，就会返回没有权限的错误 (在工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 precheck 验证权限)。如果有权限，打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。

#### 2. redo log、binlog、undo log的区别

| 特性         | redo log             | binlog                  | undo log                   |
| ------------ | -------------------- | ----------------------- | -------------------------- |
| **所属层次** | InnoDB存储引擎       | MySQL服务层             | InnoDB存储引擎             |
| **记录内容** | 数据页的物理修改     | 逻辑操作（SQL或行变化） | 数据修改前的旧值           |
| **存储位置** | 磁盘文件（循环写入） | 磁盘文件（追加写入）    | 共享表空间或独立undo表空间 |
| **主要作用** | 崩溃恢复             | 主从复制、数据恢复      | 事务回滚、MVCC             |
| **生命周期** | 数据页刷盘后可覆盖   | 永久保存（需手动清理）  | 事务结束后可清理           |

#### 3. 执行器和InnoDB引擎在执行update语句时的流程

> mysql> update T set c = c + 1 where ID = 2;

1. 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回
2. 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据
3. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务
4. 执行器生成这个操作的 binlog，并把 binlog 写入磁盘
5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成

#### 4. 数据库为什么能够恢复到半个月内任意一秒的状态？（两阶段提交、binlog）

当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，需要找回数据，那你可以这么做：

- 首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库；
- 然后，从备份的时间点开始，将备份的 binlog 依次取出来，重放到中午误删表之前的那个时刻。

做到这一切的保证就是两阶段提交：redo log的写入拆成了两个步骤：prepare和commit。第一次写入redo log后，redo log处于prepare状态，然后写入bin log，最后提交事务，redo log处于commit状态。

#### 5. Sql的优化

1. sql尽量使用索引,而且查询要走索引
2. 对sql语句优化
   1. 子查询变成left join
   2. limit 分布优化，先利用ID定位，再分页
   3. or条件优化，多个or条件可以用union all对结果进行合并（union all结果可能重复）
   4. 不必要的排序
   5. where代替having,having 检索完所有记录，才进行过滤
   6. 避免嵌套查询
   7. 对多个字段进行等值查询时，联合索引

#### 6. MySQL记录存储(Page)

- 页头：记录页面控制信息，56字节，包括页的左右兄弟页面指针、页面空间使用情况等
- 记录：

  - 最大虚记录：比页内最大主键还大
  - 最小虚记录：比页内最小主键还小
- 记录堆：行记录存储区，分为有效记录和已删除记录两种
- 自由空间链表：已删除记录组成的链表
- 未分配空间：页面未使用空间
- Slot区：slot是一些页面有效记录的指针
- 页尾：页面最后部分、8个字节，主要存储页面的校验信息

#### 7. 怎么删除表的前10000行

- 第一种方式：直接执行delete from T limit 10000，但是单个语句占用时间长，锁的时间也比较长，而且大事务还会导致主从延迟
- 第二种方式：在一个连接里循环执行20次delete from T limit 500
- 第三种方式：在20个连接里执行delete from T limit 500，会认为造成锁冲突

#### 8. B-树和B+树的区别

- B-树内部节点是保存数据的;而B+树内部节点是不保存数据的，只作索引作用，它的叶子节点才保存数据。
- B+树相邻的叶子节点之间是通过链表指针连起来的，B-树却不是
- 查找过程中，B-树在找到具体的数值以后就结束，而B+树则需要通过索引找到叶子结点中的数据才结束
- B-树中任何一个关键字出现且只出现在一个结点中，而B+树可以出现多次

#### 9. 什么是change buffer？什么时候用？怎样用收益大？

- 当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InnoDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。
- 对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。比如，要插入 (4,400) 这个记录，就要先判断现在表中是否已经存在 k=4 的记录，而这必须要将数据页读入内存才能判断。如果都已经读入到内存了，那直接更新内存会更快，就没必要使用 change buffer 了。因此，唯一索引的更新就不能使用 change buffer，实际上也只有普通索引可以使用。change buffer 用的是 buffer pool 里的内存，因此不能无限增大。change buffer 的大小，可以通过参数 innodb_change_buffer_max_size 来动态设置。这个参数设置为 50 的时候，表示 change buffer 的大小最多只能占用 buffer pool 的 50%。
- 因为 merge 的时候是真正进行数据更新的时刻，而 change buffer 的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做 merge 之前，change buffer 记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。因此，对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。反过来，假设一个业务的更新模式是写入之后马上会做查询，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代价。change buffer 反而起到了副作用。

#### 10. mysql为什么有时候查询突然慢一下？什么时候刷脏页（flush）？

1. redo log写满了，需要将一部分redo log写入到磁盘
2. 内存满了，需要淘汰一些数据页，如果是脏页就要先将脏页写到磁盘
3. mysql认为系统空闲的时候
4. mysql正常关闭的时候

#### 11. count(pk/*/1/字段)的区别

count(pk/*/1)统计的是符合条件的数据库表的行数，count(字段)统计的是符合条件的且列值不为NULL的数据库表的行数

字段有索引时：count(*) ≈count(1)>count(字段)>count(pk)。有索引的话，count(字段)会走二级索引，二级索引存储数据比主键索引少

字段无索引时：count(*) ≈count(1)>count(pk)>count(字段)。count(字段)无法走索引了，而主键还能走主键索引

count(1)不需要取出字段统计，就用常量1统计，count(字段)需要取出字段并且过滤掉NULL值。理论上count(1)更快。

count(*)内部做了很多优化，不取值，按行累加，效率很高

#### 12. mysql各种join的区别

4种join：inner join, left join, right join, cross join

- inner join
  - 定义：`INNER JOIN`只返回两个表中能够匹配上的记录。
  - 特点：
    - 如果左表和右表的关联条件匹配成功，则返回对应的记录。
    - 如果某条记录在其中一个表中没有匹配到，则该记录不会出现在结果集中。
- left join
  - 定义：`LEFT JOIN`会返回左表中的所有记录，即使右表中没有匹配的记录。
  - 特点：
    - 如果右表中没有匹配的记录，则右表的字段会以`NULL`填充。
    - 左表的所有记录都会出现在结果集中。
- right join
  - 定义：`RIGHT JOIN`会返回右表中的所有记录，即使左表中没有匹配的记录。
  - 特点：
    - 如果左表中没有匹配的记录，则左表的字段会以`NULL`填充。
    - 右表的所有记录都会出现在结果集中。
    - 通常我们更常用`LEFT JOIN`，因为可以通过调整表的顺序达到同样的效果
- cross join
  - 定义：`CROSS JOIN`会返回两个表的笛卡尔积，即左表的每一行与右表的每一行进行组合。
  - 特点：
    - 结果集的行数等于左表行数乘以右表行数。
    - 如果没有指定`ON`条件，默认就是笛卡尔积。
- FULL OUTER JOIN（**mysql中没有，特殊实现**）
  - 定义：`FULL OUTER JOIN`会返回左表和右表中的所有记录。如果某条记录在其中一个表中没有匹配，则另一个表的字段会以`NULL`填充。
  - 特点：
    - 结果集包含左表和右表的所有记录，无论是否匹配。
    - MySQL本身并不直接支持`FULL OUTER JOIN`，但可以通过`LEFT JOIN`和`RIGHT JOIN`结合`UNION`来实现。

#### 13. 主从架构

- 基本概念：
  - **主库**：负责处理所有的写操作（INSERT、UPDATE、DELETE），并将这些操作记录到二进制日志（binlog）中。
  - **从库**：负责读取主库的binlog，并在本地重放这些操作，从而实现与主库的数据同步。从库通常用于分担主库的查询压力（读写分离）或作为备份节点。
- 工作具体流程：
  1. 主库记录binlog
     - 当主库执行写操作时，会将这些操作记录到binlog中。binlog有两种格式：
       - **ROW**：记录每一行数据的变化。
       - **STATEMENT**：记录SQL语句本身。
       - **MIXED**：根据情况自动选择上述两种格式。
  2. 从库连接主库
     - 从库通过IO线程连接到主库，并请求获取主库的binlog。
     - 主库会启动一个dump线程，将binlog发送给从库。
  3. 从库保存relay log
     - 从库接收到主库的binlog后，将其保存到本地的relay log（中继日志）中。
  4.  从库重放relay log
     - 从库启动SQL线程，读取relay log中的内容，并在本地重放这些操作，从而实现与主库的数据同步。
- 存在的问题：
  - **单点故障**：主库仍然是单点，如果主库宕机，需要手动切换到从库。
  - **异步延迟**：主从同步是异步的，可能会导致从库数据落后于主库。
  - **复杂性**：手动故障切换和数据一致性校验增加了运维的复杂性。
- 更加高可用的方案：
  - MHA（Master High Availability）：可以实现自动化的主从切换
  - 半同步复制（Semi-Sync Replication）：在主从复制基础上的一种改进，确保至少有一个从库接收到binlog后，主库才会认为事务提交成功。
  - MySQL Group Replication：所有节点组成一个组，每个节点都可以接受写操作。写操作通过Paxos协议在组内传播，确保数据一致性。
  - Percona XtraDB Cluster（PXC）（不了解别说）：所有节点之间通过Galera协议进行数据同步。写操作会在所有节点上同步执行。
  - 引入中间件（如ProxySQL、MyCat）：中间件负责将写请求路由到主库，将读请求分发到从库。中间件还可以监控主库状态，并在主库故障时自动切换到从库。

### InnoDB

#### 1. InnoDB内存结构

##### 缓冲池BufferPool

- 用于加速数据的访问和修改，通过将热点数据缓存在内存的方法，最大限度地减少磁

  盘 IO，加速热点数据读写。

- 默认大小128M，Buffer Pool 中数据**以页为存储单位**，其实现的数据结构是**以页为单位的单链**

  **表**。

- 由于内存的空间限制，Buffer Pool 仅能容纳最热点的数据

- Buffer Pool 使用LRU算法（Least Recently Used 最近最少使用）淘汰非热点数据页。

  - LRU：根据页数据的历史访问来淘汰数据，**如果数据最近被访问过，那么将来被访问的几率也**

    **更高**，优先淘汰最近没有被访问到的数据。

- 对于 Buffer Pool 中数据的查询，InnoDB 直接读取返回。对于 Buffer Pool 中数据的修改，

  InnoDB 直接在 Buffer Pool 中修改，并将修改写入redo log。

- buffer pool中的数据以页为存储单位，数据结构是单链表

##### change buffer

- 用于加速非热点数据中二级索引的写入操作
- 对二级索引的修改操作会录入到redo log中
- 缓冲到一定量或者系统空闲时进行merge操作（写入磁盘）
- 在系统表空间中有相应的持久化区域
- 其物理结构为一颗名为ibuf的B+树

##### 自适应哈希索引Adaptive Hash Index

- 用于实现对于热数据页的一次查询，是建立在索引之上的索引
- 作用：对频繁查询的数据页和索引页进一步提速
- 大小为buffer pool的1/64
- 若二级索引命中AHI
  - 从AHI获取索引页记录指针，再根据主键沿着聚簇索引查找数据
- 若聚簇索引命中AHI
  - 直接返回目标数据页的记录指针，根据记录指针可以直接定位数据页

##### 日志缓冲log buffer

- innodb使用log buffer来缓冲文件的写入操作
- 内存写入加上日志文件顺序写使得innodb日志写入性能极高

#### 2. InnoDB磁盘结构

在磁盘中，InnoDB 将所有数据都逻辑地存放在一个空间中，称为表空间（Tablespace）。表空间由段（Segment）、区（extent）、页（Page）组成。开启独立表空间innodb_file_per_table=1，每张表的数据都会存储到一个独立表空间，即表名.ibd 文件。关闭独占表空间innodb_file_per_table=0，则所有基于InnoDB存储引擎的表数据都会记录到系统表空间，即 ibdata1 文件

##### 系统表空间

系统表空间是 InnoDB 数据字典、双写缓冲、修改缓冲和回滚日志的存储位置，如果关闭独立表空间，

它将存储所有表数据和索引。它默认下是一个初始大小 12MB、名为 ibdata1 的文件，系统表空间所对应的文件由

innodb_data_file_path 定义。指定系统表空间文件自动增长后，其增长大小由 innodb_autoextend_increment 设置（默认为64MB）且不可缩减，即使删除系统表空间中存储的表和索引，此过程释放的空间仅仅是在表空间文件中标记为已释放而已，并不会缩减其在磁盘中的大小。

- 数据字典（Data Dictionary）： 数据字典是由各种表对象的元数据信息（表结构，索引，列信息等）组成的内部表
- 双写缓冲（Doublewrite Buffer）：双写缓冲用于保证写入磁盘时页数据的完整性，防止发生部分写失效问题。
- 修改缓冲（Change Buffer）： 内存中 Change Buffer 对应的持久化区域
- 回滚日志（Undo Log）：实现事务进行 回滚 操作时对数据的恢复。是实现多版本并发控制（MVCC）重要组成。在事务篇详细讲解

##### 独立表空间

- 独立表空间用于存放每个表的数据和索引。其他类型的信息，如：回滚日志、双写缓冲区、系统事务信息、修改缓冲等仍存放于系统表空间内。因此即使用了独立表空间，系统表空间也会不断增长。在5.7版本中默认开启
- 开启独立表空间（File-per-table TableSpace）（ innodb_file_per_table=ON ）之后，InnoDB 会为每个数据库单独创建子文件夹，数据库文件夹内为每个数据表单独建立一个表空间文件 table.ibd 。同时创建一个 table.frm 文件用于保存表结构信息。
- 每个独立表空间的初始大小是 96KB。

##### 通用表空间

通用表空间（General Tablespace）是一个由 CREATE TABLESPACE 命令创建的共享表空间，创建时必
须指定该表空间名称和 ibd 文件位置，ibd 文件可以放置于任何 MySQL 有权限的地方。该表空间内可以
容纳多张数据表，同时在创建时可以指定该表空间所使用的默认引擎。
通用表空间存在的目的是为了在系统表空间与独立表空间之间作出平衡。系统表空间与独立表空间中的
表可以向通用表空间移动，反之亦可，但系统表空间中的表无法直接与独立表空间中的表相互转化。

##### 回滚表空间

Undo TableSpace 用于存放一个或多个 undo log 文件。默认 undo log 存储在系统表空间中，MySql
5.7中支持自定义 Undo log 表空间并存储所有 undo log。一旦用户定义了 Undo Tablespace，则系统
表空间中的 Undo log 区域将失效。对于 Undo Tablespace 的启用必须在 MySQL 初始化前设置，
Undo Tablespace 默认大小为 10MB。Undo Tablespace 中的 Undo log 表可以进行 truncate 操作。

##### 临时表空间

MySQL 5.7 之前临时表存储在系统表空间中，这样会导致 ibdata 在使用临时表的场景下疯狂增长。5.7
版本之后 InnoDB 引擎从系统表空间中抽离出临时表空间（Temporary Tablespace），用于独立保存
临时表数据及其回滚信息。该表空间文件路径由 innodb_temp_data_file_path 指定，但必须继承
innodb_data_home_dir 。

#### 3. 表空间存储结构

##### 段

表空间由各个段（Segment）组成，创建的段类型分为数据段、索引段、回滚段等。由于 InnoDB 采用
聚簇索引与 B+ 树的结构存储数据，所以事实上数据页和二级索引页仅仅只是 B+ 树的叶子节点，因此数
据段称为 Leaf node segment，索引段其实指的是 B+ 树的非叶子节点，称为 Non-Leaf node
segment。一个段会包含多个区，至少会有一个区，段扩展的最小单位是区。

- 数据段称为 Leaf node segment
- 索引段称为 Non-Leaf node segment

##### 区

区（Extend）是由连续的页组成的空间，大小固定为 1MB，由于默认页大小为 16K，因此一个区默认存储 64 个连续的页。如果页大小调整为 4K，则 256 个连续页组成一个区。为了保证页的连续性，InnoDB 存储引擎会一次从磁盘申请 4 ~ 5 个区。

##### 页

- 页（Page）是 InnoDB 的基本存储单位，每个页大小默认为 16K
- 操作系统读写磁盘最小单位是页，4k
- 磁盘存储数据量最小单位512byte

##### 行

- innodb的数据是以行为单位存储、一个页中包含多个行
- innodb提供4种行格式
  - compact
  - redundant
  - dynamic
  - compressed
- 默认行格式为dynamic

### 事务

#### 1. 事务四大特性ACID

1. 原子性A，要么全部执行，要么全部不执行
2. 一致性C，事务前后数据的完整性必须保持一致，例如，转账完成后，A账户和B账户的总金额应该和转账前保持一致，不会凭空多出或减少资金。
3. 隔离性I，多个事务并发执行时，批次事务操作数据不能互相干扰，所以要隔离。
4. 持久性D，一旦事务提交，对数据的改变就是永久的

#### 2. 事务的隔离级别

1. 读未提交RU：Read Uncommitted 一个事务还没提交时，它做的变更就能被别的事务看到（可能发生脏读、不可重复读和幻读问题）
2. 读已提交RC：Read committed 一个事务提交之后，它做的变更才会被其他事务看到（可能发生不可重复读和幻读问题）
3. 可重复读RR：Repeatable Read 一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的（可能发生幻读问题），但其实mysql已经将幻读问题通过其他方式解决
4. 可串行化：Serializable 同时只能执行一个事务，相当于事务中的单线程

#### 3. 事务并发可能出现的情况

1. 脏读：指一个事务读取到另一个事务未提交的数据
2. 不可重复读：指一个事务读到另一个事务已经update的数据，引发事务中的多次查询结果不一致
3. 虚读/幻读：一个事务读到另一个事务已经Insert的数据，导致事务中多次查询的结果不一致

#### 4. 什么是MVCC？

- MVCC全称叫多版本并发控制，是RDBMS常用的一种并发控制方法，用来对数据库数据进行并发访问，实现事务。核心思想：读不加锁，读写不冲突
- 实现原理：数据快照，不同事务访问数据快照中不同版本的数据
- 关键要素：Undo log和Read View

#### 5. undo log

- Insert Undo log：是在Insert操作中产生的Undo日志。Insert 操作的记录只对事务本身可见，对于其它事务此记录是不可见的，所以 Insert Undo Log 可以在事务提交后直接删除而不需要进行回收操作。
- Update Undo log：是Update或Delete 操作中产生的Undo日志。Update操作会对已经存在的行记录产生影响，为了实现MVCC多版本并发控制机制，因此Update Undo日志不能在事务提交时就删除，而是在事务提交时将日志放入指定区域，等待 Purge 线程进行最后的删除操作。

#### 6. Read View

ReadView是张存储事务id的表，主要包含当前系统中有哪些活跃的读写事务，把它们的事务id放到一个列表中。结合Undo日志的默认字段【事务trx_id】来控制那个版本的Undo日志可被其他事务看见。

##### 四个列：

- m_ids：表示在生成ReadView时，当前系统中活跃的读写事务id列表
- m_low_limit_id：事务id下限，表示当前系统中活跃的读写事务中最小的事务id，m_ids事务列表中的最小事务id
- m_up_limit_id：事务id上限，表示生成ReadView时，系统中应该分配给下一个事务的id值
- m_creator_trx_id：表示生成该ReadView的事务的事务id

##### 生成时机

- 开启事务之后，在第一次查询(select)时，生成ReadView
- RC 和 RR 隔离级别的差异本质是因为MVCC中ReadView的生成时机不同，详细生成时机在案例中分析

#### 7. 事务隔离机制（可重复读）

##### 事务的启动时机

- 第一种启动方式begin/start transaction，一致性视图是在执行第一个快照读语句时创建的；
- 第二种启动方式，一致性视图是在执行 start transaction with consistent snapshot 时创建的

##### 数据的row trx_id

每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把 transaction id 赋值给这个数据版本的事务 ID，记为 row trx_id 

##### 一致性视图read view

innodb为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务 ID。“活跃”指的就是，启动了但还没提交。数组里面事务 ID 的最小值记为低水位，当前系统里面已经创建过的事务 ID 的最大值加 1 记为高水位。这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）

##### 数据版本可见性规则

- 如果小于低水位，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的；
- 如果大于高水位，表示这个版本是由将来启动的事务生成的，是肯定不可见的；
- 如果处在低水位和高水位之间，那就包括两种情况
  - a. 若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见；
  - b. 若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。

##### 数据的查询

一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况：

1. 版本未提交，不可见；
2. 版本已提交，但是是在视图创建后提交的，不可见；
3. 版本已提交，而且是在视图创建前提交的，可见。

##### 当前读和快照读

快照读：读不加锁，读取的是版本链的快照数据，默认的读都是快照读

```mysql
select * from t where ?
```

当前读：更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。

除了 update 语句外，select 语句如果加锁，也是当前读。

```mysql
select k from t where id=1 lock in share mode;（读锁，s锁，共享锁）
select k from t where id=1 for update;（写锁，x锁，徘他锁）
insert into t values ();（写锁）
update t set ? where ?;（写锁）
delete from table where ?;（写锁）
```

##### 可重复读和读已提交这俩个隔离级别的区别

- 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图；
- 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。

##### 实战

**可重复读隔离级别**

```mysql
CREATE TABLE `t` (
  `id` int(11) NOT NULL,
  `k` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB;
insert into t(id, k) values(1,1),(2,2);
```



| 事务A                                       | 事务B                                       | 事务C                          |
| ------------------------------------------- | ------------------------------------------- | ------------------------------ |
| start transaction with consistent snapshot; |                                             |                                |
|                                             | start transaction with consistent snapshot; |                                |
|                                             |                                             | update t set k=k+1 where id=1; |
|                                             | update t set k=k+1 where id=1;              |                                |
|                                             | select k from t where id=1;                 |                                |
| select k from t where id=1;                 |                                             |                                |
| commit;                                     |                                             |                                |
|                                             | commit;                                     |                                |

结果：事务B查询到的k是3，事务A查询到的k是1

| 事务A                                       | 事务B                                       | 事务C                                       |
| ------------------------------------------- | ------------------------------------------- | ------------------------------------------- |
| start transaction with consistent snapshot; |                                             |                                             |
|                                             | start transaction with consistent snapshot; |                                             |
|                                             |                                             | start transaction with consistent snapshot; |
|                                             |                                             | update t set k=k+1 where id=1;              |
|                                             | update t set k=k+1 where id=1;              |                                             |
|                                             | select k from t where id=1;                 |                                             |
|                                             |                                             | commit;                                     |
| select k from t where id=1;                 |                                             |                                             |
| commit;                                     |                                             |                                             |
|                                             | commit;                                     |                                             |

由于两阶段锁协议，事务C还没释放锁，所以事务B得等到事务C释放之后才能继续它的当前读。

**读已提交隔离级别**

| 事务A                       | 事务B                          | 事务C                          |
| --------------------------- | ------------------------------ | ------------------------------ |
| begin;                      |                                |                                |
|                             | begin;                         |                                |
|                             |                                | update t set k=k+1 where id=1; |
|                             | update t set k=k+1 where id=1; |                                |
|                             | select k from t where id=1;    |                                |
|                             | **read view**                  |                                |
| select k from t where id=1; |                                |                                |
| **read view**               |                                |                                |
| commit;                     |                                |                                |
|                             | commit;                        |                                |

由于是读已提交的隔离级别，read view的创建时机不同，事务A看到的k是2，事务B看到的是3。

##### 很多时候将隔离级别改为读已提交，为什么

为什么选“读已提交”

- **减少锁竞争，提升并发性能**：在“可重复读”隔离级别下，MySQL通过间隙锁（Gap Lock）和Next-Key Lock来防止幻读问题。这些锁机制虽然能够保证更高的隔离性，但也会导致更多的锁竞争，尤其是在高并发的写操作场景中。在“读已提交”隔离级别下，MySQL不会使用间隙锁，读操作只会对当前已经提交的数据加共享锁，这大大减少了锁的竞争，从而提升了系统的并发能力。
- **避免不必要的事务回滚**：在“可重复读”隔离级别下，由于锁范围较大，可能导致死锁或事务回滚的风险增加。例如，两个事务同时尝试更新同一行数据时，可能会因为间隙锁而互相阻塞，最终触发死锁检测机制导致事务回滚。使用“读已提交”可以减少这种风险，因为它的锁粒度更小，锁持有时间更短，事务冲突的概率也随之降低。
- **业务需求和一致性妥协**：业务需求上不需要强一致性

低隔离级别的好处：

- **性能提升**：锁粒度更小、减少死锁、减少回滚开销
- **资源占用减少**：高隔离级别通常需要维护更多的元数据信息（例如多版本控制中的历史版本数据）
- **灵活性更好**：在某些业务场景中，允许一定程度的“脏读”或“不可重复读”是可以接受的，比如缓存刷新、实时监控等场景。低隔离级别能够更好地适应这些场景的需求，而不需要牺牲过多的性能。

典型场景：

- 实时监控
- 日志记录
- 缓存刷新与预热

### 索引

#### 1. 索引的常见模型

- Hash：适用于只有等值查询的场景，比如Memcached及其一些NoSQL引擎
- 有序数组：在等值查询和范围查询场景中的性能就都非常优秀。只适用于静态存储引擎。
- 二叉搜索树：查询是O(logN)，维护它是一颗平衡二叉树也需要O(logN)，数据库大多不适用二叉树，树高过高，会使用N叉树

#### 2.InnoDB的索引模型

- 在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。又因为前面我们提到的，InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的
- 每一个索引在 InnoDB 里面对应一棵 B+ 树。

#### 3. 主键索引和普通索引的区别

- 主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引
- 非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引
- 如果语句是 select * from T where ID=500，即主键查询方式，则只需要搜索 ID 这棵 B+ 树
- 如果语句是 select * from T where k=5，即普通索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表

#### 4. 聚簇和非聚簇索引的区别是什么

- 聚簇索引：找到了索引就找到了需要的数据，那么这个索引就是聚簇索引，所以主键就是聚簇索引，修改聚簇索引其实就是修改主键。
- 非聚簇索引：索引的存储和数据的存储是分离的，也就是说找到了索引但没找到数据，需要根据索引上的值(主键)再次回表查询,非聚簇索引也叫做辅助索引
- MySQL中Innodb中两者都有，而myisam只有非聚簇索引

#### 5. 什么是覆盖索引

- 如果执行的语句是 select ID from T where k between 3 and 5，这时只需要查 ID 的值，而 ID 的值已经在 k 索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引 k 已经“覆盖了”我们的查询需求，我们称为覆盖索引。该操作能用于性能优化。

#### 6. 什么是最左前缀

- 联合索引的最左N个字段，也可以是字符串索引的最左M个字符

#### 7. 在建立联合索引的时候，如何安排索引内的字段排序

- 这里我们的评估标准是，索引的复用能力。因为可以支持最左前缀，所以当已经有了 (a,b) 这个联合索引后，一般就不需要单独在 a 上建立索引了。因此，第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。

#### 8. 什么是索引下推

- 是组合索引查询优化的手段

- ```mysql
  show variables like 'optimizer_switch';
  
  -- 开启
  set optimizer_switch = 'index_condition_pushdown=on';
  ```

- like 'hello%' and age > 10检索，MySQL5.6之前，会对匹配的数据进行回表查询。5.6之后，会先过滤掉age < 10的数据，再进行回表查询，减少回表率，提升检索速度。

- 举例：现在有(a,b,c)的联合索引，使用select * from t where a=13 and b>15 and c=5 and d=6;走(a,b)索引，然后过滤掉c!=5的数据之后再去回表，减少回表率

#### 9. 索引在什么情况下失效

- 在索引列上做了函数运算，不过mysql8之后可以加函数索引解决这个问题
- 没有符合最左匹配法则，联合索引和like这两种情况
- 索引列出现隐式转换的时候，比如索引列是字符串，但是sql查询里没有使用引号
- 使用or，但是前后没有同时使用索引，可以使用union来改进
- 使用不等于 !=或者<>
- 使用is null或者is not null
- 使用explain来看到底走了哪些索引

#### 10. mysql的索引有哪些

- FULLTEXT：全文索引，倒排索引
- HASH：哈希索引，适合等值查询，不适合范围查询
- BTREE：B+树，
- RTREE：优势在于范围查找

#### 11. 唯一索引和普通索引怎么选择

- 这两者在查询上没有差别，主要是更新性能的影响。尽量使用普通索引
- 如果所有更新后面，马上要查，那么应该关闭change buffer
- 实际中，普通索引和change buffer配合使用

#### 12. 索引的设计原则

- 代码先行，索引后上。
- 选择合适的列。选择经常用于查询、连接、过滤、排序的列作为索引列
- 联合索引尽量覆盖业务上多个列
- 尽量使用覆盖索引
- 避免过长索引。长字符串可以用前缀索引
- 选择适当的索引类型。如普通索引、唯一索引、全文索引等
- 如果有排序，考虑在排序的列上加索引
- 避免冗余索引
- 考虑区分度。尽量不要挑选区分度不高的字段作为索引
- 定期维护索引。删除不再使用的索引、重建碎片化的索引
- 多表join关联查询on两边的字段
- 频繁更新的不建议使用索引

#### 13. mysql为什么选错索引

- 数据库统计索引区分度不准，使用采样统计，导致优化器判断使用不同情况下的预计扫描行不准确

  - 解决：可以使用analyze table tableName 来让MySQL重新统计

- sql有排序或使用临时表，优化器容易因考虑其他因素反而选错了索引

  - 解决：1、使用force index强行使用某个索引

  - 2、修改SQL语句引导优化器选择正确索引。例如：

    ```mysql
    select * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b limit 1;
    
    select * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b,a limit 1;
    ```

  - 3、增加或删除索引，从而使用正确索引

#### 14. 如何给字符串字符段加索引

- 直接创建完整索引，这样可能比较占用空间
- 创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引
- 倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题，不支持范围扫描
- 创建 hash 字段索引，查询性能稳定，有额外的存储和计算消耗，不支持范围扫描，例如身份证字段，可以创建一个整数字段存身份证的hash值，在这个整数字段上创建索引

### 锁

#### 1. MySQL的锁

- 按锁功能划分：
  - 共享锁 shard lock S锁 读锁：读锁之间是共享的，读锁之间互相不阻塞 select * from t lock in share mode
  - 排他锁 exclusive lock x锁 写锁：写锁是排他的，写锁阻塞其他的读和写锁 select * from t for update

- 按粒度划分：
  - 全局锁：全局锁就是对整个数据库实例加锁。MySQL 提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。典型使用场景是做全库逻辑备份。对于全部是InnoDB引擎的库，建议使用-Single-transaction参数，这里是使用事务的可重复读隔离级别，使用后能开启一个事务，确保拿到一致性视图。
  - 表级锁：MySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。与 FTWRL (Flush tables with read lock)类似，可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。表锁一般用在数据库引擎不支持行锁的时候才被用到。
    - 表读锁 table read lock：阻塞对当前表的写，但不阻塞读
    - 表写锁 table write lock：阻塞对当前表的读和写
    - 元数据锁 meta data lock：不需要显式指定，在访问表时会被自动加上，作用保证读写的正确性
      - 当对表做增删改查操作的时候加元数据读锁
      - 当对标做结构变更操作的时候加元数据写锁

    - 自增锁 auto-inc locks：特殊的表级锁，自增列事务性插入操作时产生

  - 行锁：MySQL 的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB 是支持行锁的，这也是 MyISAM 被 InnoDB 替代的重要原因之一。
    - 记录锁 record locks：锁定索引中一条记录
    - 间隙锁 gap locks：仅仅锁住一个索引区间
    - 临建锁 next-key locks：记录锁和间隙锁的组合，解决幻读的问题
    - 插入意向锁 insert intention locks：做insert时添加的对记录id的锁
    - 意向锁：存储引擎级别的“表级”锁


#### 2. 如何安全地给小表加字段

在MySQL5.5版本引入了元数据锁（meta data lock，MDL），对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。

如果有一个类似alter table t add f int；这样的语句被阻塞了，后续所有的语句都会被阻塞，等于这个表完全不可读写了。

首先我们要解决长事务，事务不提交，就会一直占着 MDL 锁。在 MySQL 的 information_schema 库的innodb_trx 表中，你可以查到当前执行中的事务。如果你要做 DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务。

比较理想的机制是，在 alter table 语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者 DBA 再通过重试命令重复这个过程。

#### 3. 两阶段锁协议

- 在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。

#### 4. 出现死锁应该怎么办

- 一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。
- 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。默认就是on。

#### 5. 怎么解决由热点行更新导致的性能问题

- 问题的原因在于死锁检测要耗费大量的CPU资源
- 方法一：假如能确保这个业务一定不会出现死锁，可以临时把死锁检错关掉
- 方法二：控制并发度。比如控制同一行最多就10个线程在更新，可以考虑使用中间件实现，或者修改MySQL源码，基本思路就是，对于相同行的更新，在进入引擎之前排队。这样在 InnoDB 内部就不会有大量的死锁检测工作了。但是如果有太多客户端同时进来，并发度也上去了。解决的基本思路就是在进入引擎之前排队。
- 方法三：你可以考虑通过将一行改成逻辑上的多行来减少锁冲突。还是以影院账户为例，可以考虑放在多条记录上，比如 10 个记录，影院的账户总额等于这 10 个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的 1/10，可以减少锁等待个数，也就减少了死锁检测的 CPU 消耗。

#### 6. innodb的行级锁

mysql的行级锁是由存储引擎实现，innodb行锁是通过给索引上的索引项加锁来实现的

只有通过索引条件检索的数据innodb才使用行级锁，否则innodb都使用表锁

按照范围分类：记录锁、间隙锁、临建锁、插入意向锁

按功能分：读锁、写锁

如何加行锁？update、delete、insert会自动给涉及的数据集加写锁；对于普通select，不会加任何锁。lock in share mode;（读锁，s锁，共享锁）；for update;（写锁，x锁，排他）

案例

```mysql
CREATE TABLE `t1_simple` (
    `id` int(11) NOT NULL,
    `pubtime` int(11) NULL DEFAULT NULL,
    PRIMARY KEY (`id`) USING BTREE,
    INDEX `idx_pu`(`pubtime`) USING BTREE
) ENGINE = InnoDB;
INSERT INTO `t1_simple` VALUES (1, 10);
INSERT INTO `t1_simple` VALUES (4, 3);
INSERT INTO `t1_simple` VALUES (6, 100);
INSERT INTO `t1_simple` VALUES (8, 5);
INSERT INTO `t1_simple` VALUES (10, 1);
INSERT INTO `t1_simple` VALUES (100, 20);
```

##### 记录锁

记录锁（Record Locks）仅仅锁住索引记录的一行，在单条索引记录上加锁。记录锁锁住的永远是索引，而非记录本身，即使该表上没有任何显示索引，那么innodb会在后台创建一个隐藏的聚簇索引索引，那么锁住的就是这个隐藏的聚簇索引索引

```mysql
-- 加记录读锁
select * from t1_simple where id = 1 lock in share mode;
-- 加记录写锁
select * from t1_simple where id = 1 for update;
-- 新增，修改，删除加记录写锁
insert into t1_simple values (1, 22);
update t1_simple set pubtime=33 where id =1;
delete from t1_simple where id =1;
```

##### 间隙锁

- 间隙锁(Gap Locks)，仅仅锁住一个索引区间（开区间，不包括双端端点）。
- 间隙锁可用于防止幻读，保证索引间隙不会被插入数据。
- 在索引记录之间的间隙中加锁，或者是在某一条索引记录之前或者之后加锁，并不包括该索引记录本身。
- 在可重复读（REPEATABLE READ）这个隔离级别下生效。

```mysql
-- session1
begin;
select * from t1_simple where id > 4 for update; -- 加间隙锁
-- 临键锁区间(4,100+)
commit;

-- session2
begin;
insert into t1_simple values (7,100); -- 阻塞
insert into t1_simple values (3,100); -- 成功
commit;
```

##### 临建锁

- 临键锁(Next-Key Locks)相当于记录锁 + 间隙锁【左开右闭区间】，例如（5,8]
- 默认情况下，innodb使用临键锁来锁定记录，但在不同的场景中会退化
  - 唯一性字段等值（=）且记录存在，退化为记录锁
  - 唯一性字段等值（=）且记录不存在，退化为间隙锁
  - 唯一性字段范围（< >），还是临键锁
  - 非唯一性字段，默认是临键锁
- 当查询的索引含有唯一属性的时候，临键锁会进行优化，将其降级为记录锁，即仅锁住索引本身，不是范围。

```mysql
-- session1
begin;
select * from t1_simple where pubtime = 20 for update;
-- 临键锁区间(10,20],(20,100]
commit;

-- session2
begin;
insert into t1_simple values (16, 19); -- 阻塞
select * from t1_simple where pubtime = 19 for update; -- 不阻塞
select * from t1_simple where pubtime = 20 for update; -- 阻塞
insert into t1_simple values (16, 50); -- 阻塞
insert into t1_simple values (16, 101); -- 成功
commit;
```

##### 插入意向锁

- 插入意向锁（Insert Intention Locks）是一种在 INSERT 操作之前设置的一种特殊的间隙锁。
- 插入意向锁表示了一种插入意图，即当多个不同的事务，同时往同一个索引的同一个间隙中插入数据的时候，它们互相之间无需等待，即不会阻塞。
- 插入意向锁不会阻止插入意向锁，但是插入意向锁会阻止其他间隙写锁（排他锁）、记录锁。

```mysql
-- session1
-- 第一次
begin;
insert into t1_simple values (60, 200);
-- 插入意向锁区间(10,100)
commit;
-- 第二次
begin;
select * from t1_simple where id > 10 for update;
-- 临键锁（区间）写锁区间(10,100+)
commit;

-- session2
-- 第一次
begin;
insert into t1_simple values (70, 300); -- 没有发生阻塞
-- 插入意向锁区间(10,100)
commit;
-- 说明两个插入意向锁之间是兼容的，可以共存!
-- 第二次
begin;
insert into t1_simple values (90, 300); -- 被阻塞，阻塞的原因在于，插入意向锁和其他写锁之间是互斥的！
commit;
```

#### 7. 行锁加锁规则

- 主键索引
  - 等值条件，命中，加记录锁
  - 等值条件，未命中，加间隙锁
  - 范围条件，命中，包含where条件的临键区间，加临键锁
  - 范围条件，没有命中，加间隙锁

- 辅助索引
  - 等值条件，命中，命中记录的辅助索引项 + 主键索引项加记录锁，辅助索引项两侧加间隙锁
  - 等值条件，未命中，加间隙锁
  - 范围条件，命中，包含where条件的临键区间加临键锁。命中记录的id索引项加记录锁
  - 范围条件，没有命中，加间隙锁

#### 8. 表锁：意向锁

InnoDB也实现了表级锁，也就是意向锁【Intention Locks】。意向锁是MySQL内部使用的，不需要用户干预。意向锁和行锁可以共存，意向锁的主要作用是为了全表更新数据时的提升性能。否则在全表更新数据时，需要先检索该范是否某些记录上面有行锁。那么将是一件非常繁琐且耗时操作。

作用：

- 表明：“某个事务正在某些行持有了锁、或该事务准备去持有锁”
- 意向锁的存在是为了协调行锁和表锁的关系，支持多粒度（表锁与行锁）的锁并存

意向锁和读锁、写锁的兼容关系

IS：意向读锁
IX：意向写锁
S：读锁
X：写锁
横着表示事务A加了哪些锁，竖着表示事务B此时再加这些锁能不能成功

|      | IS   | IX   | S    | X    |
| ---- | ---- | ---- | ---- | ---- |
| IS   | 是   | 是   | 是   | 否   |
| IX   | 是   | 是   | 否   | 否   |
| S    | 是   | 否   | 是   | 否   |
| X    | 否   | 否   | 否   | 否   |

#### 9. 悲观锁和乐观锁

- 乐观锁
  - 定义：乐观锁的核心思想是假设数据在大多数情况下不会发生冲突，因此在操作数据时不会加锁，而是在提交更新时才会检查在此期间是否有其他事务修改过该数据。如果发现有冲突，则采取相应的措施（比如重试或者抛出异常）。
  - 使用场景：
    - **电商系统中的库存扣减**：对于一些热销商品，可能大部分时间都在查询库存，真正扣减库存的操作非常少。
    - **用户信息更新**：用户的个人信息（如昵称、头像等）一般不会有频繁的并发修改需求，这种场景下可以使用乐观锁。
    - **分布式系统中的幂等性处理**：在某些分布式系统中，为了保证操作的幂等性，可以通过乐观锁来避免重复提交导致的数据不一致问题。
  - 实现方式：通过版本号字段或时间戳字段
- 悲观锁
  - 定义：悲观锁的核心思想是假设数据会发生冲突，因此在整个数据操作过程中会对数据加锁，确保其他事务无法修改该数据，直到当前事务完成。
  - 使用场景：
    - **金融交易系统**：转账、支付等操作对数据一致性要求非常高，且并发冲突较多，适合使用悲观锁。
    - **库存扣减（高频写场景）**：对于一些秒杀场景，库存扣减操作非常频繁，使用悲观锁可以有效避免超卖问题。
    - **复杂业务逻辑**：如果某个事务涉及多个步骤且需要保证强一致性，使用悲观锁可以减少中间状态被其他事务干扰的风险。
  - 实现方式：
    - 悲观锁通常通过数据库的锁机制实现

| 特性         | 乐观锁                   | 悲观锁                          |
| ------------ | ------------------------ | ------------------------------- |
| **核心思想** | 假设冲突较少，提交时检查 | 假设冲突较多，操作前加锁        |
| **性能**     | 高（无锁操作）           | 较低（有锁操作可能导致阻塞）    |
| **适用场景** | 读多写少，冲突概率低     | 写多读少，冲突概率高            |
| **实现方式** | 版本号/时间戳            | 数据库锁机制（如 `FOR UPDATE`） |

### 实战

#### 1. 现在有学生表、科目表、成绩表，求总分最高的学生和单科分数最高的学生

```mysql
student(id, name)
course(id, name)
score(id, course_id, student_id, score)

-- 在分数表求最大分数
select sum(score) from score group by student_id order by sum(score) desc limit 1;

-- 求分数表最大分数的student_id
select distinct(student_id), sum(score) from score group by student_id having sum(score) = (select sum(score) from score group by student_id order by sum(score) desc limit 1);

-- 求总分最高的学生 
select s.name, t.score from student s right join (select distinct(student_id), sum(score) score from score group by student_id having sum(score) = (select sum(score) from score group by student_id order by sum(score) desc limit 1)) t on s.id = t.student_id;

-- 在分数表单科最高分
select max(score) max_score, course_id from score group by course_id;

-- 在分数表单科最高分的学生id
select student_id, s1.course_id, s2.max_score from score s1 right join (select max(score) max_score, course_id from score group by course_id) s2 on s1.course_id = s2.course_id and s1.score = s2.max_score;

-- 求单科分数最高的学生
select s.name, c.name, s2.max_score from score s1 right join (select max(score) max_score, course_id from score group by course_id) s2 on s1.course_id = s2.course_id and s1.score = s2.max_score left join course c on c.id = s1.course_id left join student s on s.id = student_id;
```

## Redis数据库

### 1. redis的底层数据结构

#### string

简单动态字符串 (Simple Dynamic String)SDS。不用c语言字段的char*，因为

1. 计算长度是O(n)
2. 对于二进制不友好，比如存储图片的二进制，可能会提前遇到字符串的结尾标志
3. 不可修改

redis的结构体

```c
struct --attribute_- ((-_packed__)) sdshdr8{
    uint8_t len;/* buf已保祥的字符串字节数，不包含结束标示*/
    uint8_t alloc;/* buf申请的总的字节数，不包含结束标示*/
    char buf [];
}

// 忽略sdshdr16、sdshdr32、sdshdr64、sdshdr5
```

比如字符串name的sds结构如下：

| len：4 | alloc：4 | n    | a    | m    | e    | \0   |
| ------ | -------- | ---- | ---- | ---- | ---- | ---- |

后面会多存一个 \0,实际存储多一个字节

#### IntSet

IntSet是Redis中set集合的一种实现方式，基于整数数组来实现，并且具备⻓
度可变、有序等特征。

```c
typedef struct intset {
    uint32_t encoding; /*编码方式，支持存放16位、32位、64位*/
    uint32_t Length;/* 元素个数 */
    int8_t contents[];/*整数数组，保存集合数据*/
}
```

假设原本有一个intset，元素是{5,10,20}，这时候存的都是16位，两个byte的数据。新增一个50000，那么会先进行扩容，两个byte扩展为4个byte，倒着扩容的，顺序是20，10，5，encoding改成INTSET_ENC_INT32，length改成4。 如果是要扩容的情况，就直接看是大于0还是小于0，小于0插最前面，大于0插最后面。如果不扩容，就二分查找法找到要插入的位置然后插入。

#### Dict

整个redis就类似于一个java1.7的hashmap。用Dict来实现

```c
typedef struct dictht {
    // entry数组 数组中保存的是指向entry的指针
    dictEntry **table;
    //哈希表大小
    unsigned Long size;
    //哈希表大小的掩码，总等于size - 1
    unsigned Long sizemask;
    // entry个数
    unsigned Long used;
}

typedef struct dictEntry {
    void *key; // 键
    union {
        void *val;
        uint64_t u64;
        int64_t s64;
        double d;
    } v;//值
    //下一个Entry的指针
    struct dictEntry *next;
}

typedef struct dict {
    dictType *type;// dict类型，内置不同的hash函数
    void *privdata; // 私有数据，在做特殊hash运算时用
    dictht ht[2];// 一个Dict包含两个哈希表，其中一个是当前数据，另一个一般是空，rehash时使用
    Long rehashidx; // rehash的进度，-1表示未进行
    int16_t pauserehash；// rehash是否暂停，1则暂停，o则继续
}
```

Dict的负载因子LoadFactor = used/size，新增时或者删除时都会检查，会进行扩容或者缩容

rehash过程：

1. 计算新hash表的realsize，即第一个大于等于dict.ht[0].used的2^n
2. 按照新的realsize申请内存空间，创建dictht，并赋值给dict.ht[1]
3. 设置rehashidx= 0，标示开始rehash
4. 将dict.ht[0]中的每一个dictEntry都rehash到dict.ht[1]
   每次执行新增、查询、修改、删除操作时，都检查一下dict.rehashidx是否大于-1，如果是则将ht[0].table[rehashidx]的entry链表rehash到dict.ht[1]，井且将rehashidx++。直至dict.ht[0]的所有数据都rehash到dict.ht[1] (渐进式hash，不会一次性就copy到新的hashTable)
5. 将dict.ht[1]赋值给dict.ht[0]，给dict.ht[1]初始化为空哈希表
6. 将rehashidx赋值为-1，代表rehash结束
7. 在rehash过程中，新增操作，则直接写入ht[1]，查询、修改和删除则会在dict.ht[0]和dict.ht[1]依次查找并执行。这样可以确保h[0]的数据只减不增，随着rehash最终为空

#### ZipList

压缩列表实际上类似于一个数组，数组中的每一个元素都对应保存一个数据。和数组不同的是，压缩列表在表头有三个字段 zlbytes、zltail 和 zllen，分别表示列表长度、列表尾的偏移量和列表中的 entry 个数；压缩列表在表尾还有一个 zlend，表示列表结束。

| zlbytes | tltail | zllen | entry1 | entry2 | ...  | entryN | zlend |
| ------- | ------ | ----- | ------ | ------ | ---- | ------ | ----- |

在压缩列表中，如果我们要查找定位第一个元素和最后一个元素，可以通过表头三个字段的长度直接定位，复杂度是 O(1)。而查找其他元素时，就没有这么高效了，只能逐个查找，此时的复杂度就是 O(N) 了。

其中entry的结构如下：

| previous_entry_length | encoding | content |
| --------------------- | -------- | ------- |

- previous_entry_length：前一节点的字节大小，占1个或5个字节。如果前一节点的⻓度小于254字节，则采用1个字节来保存这个⻓度值。如果前一节点的⻓度大于254字节，则采用5个字节来保存这个⻓度值，第一个字节为0xfe，后4个字节才是真实⻓度数据
- encoding：编码属性，记录content的数据类型（字符串还是整数）以及content的字节数，占用1个、2个或5个字节
- Content：负责保存节点的数据，可以是字符串或整数

zipList可能导致的问题：连锁更新(Cascade Update）

如果每个entry都是250个字节，那么previous_entry_length只需要一个字节存就可以了，而如果恰好在最前面插入一个254字节的数据，就会导致后续的每一个entry的previous_entry_length都要从1个字节变成5个字节。

#### QuickList

zipList需要连续内存空间，存储数据也不能太多，怎么办？quickList是一个有前后指针的列表，每个node就是一个zipList，相当于就是把数据打碎了，但不是很碎，然后用一个有前后指针的列表来管理。能够设置每个zipList的大小，也能设置前后各有多少个zipList不压缩，其余压缩（更加节省空间）。

#### SkipList

有序链表只能逐一查找元素，导致操作起来非常缓慢，于是就出现了跳表。具体来说，跳表在链表的基础上，增加了多级索引，通过索引位置的几个跳转，实现数据的快速定位。结构如下：

```c
typedef struct zskiplist {
    //头尾节点指针
    struct zskiplistNode *header, *tail;
    //节点数量
    unsigned Long Length;
    // 最大的索引层级，默认是1
    int Level;
} zskiplist;

typedef struct zskiplistNode {
    sds ele; //节点存储的值
    double score;// 节点分数，排序、查找用
    struct zskiplistNode *backward;// 前一个节点指针
    struct zskiplistLevel {
        struct zskiplistNode * forward;//下一个节点指针
        unsigned Long span;//索引跨度
    } level[];//多级索引数组
} zskiplistNode;
```

#### redisObject

redis的value都是redisObjec，管你什么类型都统统封装成redisObject，结构如下：

```c
typedef struct redisObject {
    unsigned type:4;
    unsigned encoding:4;
    unsigned lru:LRU_BITS;
    int refcount;
    void *ptr;
} robj;
```

type：对象类型，分别是string、hash、list、set、zset，占4个bit位

encoding：底层编码方式，共有11种，占4个bit

lru：表示该对象最后一次被访问的时间，其占用24个bit

refcount：对象引用计数器

\*ptr：指针，指向实际存放数据的空间

| 编码方式                | 说明                 |
| ----------------------- | -------------------- |
| OBJ_ENCODING_RAW        | raw编码动态字符串    |
| OBJ_ENCODING_INT        | long类型的整数字符串 |
| OBJ_ENCODING_HT         | hash表（dict）       |
| OBJ_ENCODING_ZIPMAP     | 废弃                 |
| OBJ_ENCODING_LINKEDLIST | 双端链表             |
| OBJ_ENCODING_INTSET     | 整数集合             |
| OBJ_ENCODING_SKIPLIST   | 跳表                 |
| OBJ_ENCODING_EMBSTR     | embstr的动态字符串   |
| OBJ_ENCODING_QUICKLIST  | 快速列表             |
| OBJ_ENCODING_STREAM     | stream流             |

| 数据类型   | 编码方式                                             |
| ---------- | ---------------------------------------------------- |
| OBJ_STRING | int、embstr、raw                                     |
| OBJ_LIST   | LinkedList和ZipList（3.2以前）、QuickList（3.2以后） |
| OBJ_SET    | intset、HT                                           |
| OBJ_ZSET   | ZipList、HT、SkipList                                |
| OBJ_HASH   | ZipList、HT                                          |

### 2. redis单线程为什么这么快？（redis网络模型）

Redis 单线程是指它对网络 IO 和数据读写的操作采用了一个线程，而 采用单线程的一个核心原因是避免多线程开发的并发控制问题。单线程的 Redis 也能获得 高性能，跟多路复用的 IO 模型密切相关，因为这避免了 accept() 和 send()/recv() 潜在的 网络 IO 操作阻塞点。

使用select/epoll等机制。select/epoll 提供了基于事件的回调机制，即针对不同事件的发生，调用相应的处理函数。这些事件会被放进一个事件队列，Redis 单线程对该事件队列不断进行处理。这样一来，Redis 无需一直轮询是否有请求实际发生，这就可以避免造成 CPU 资源浪费。同时，Redis 在对事件队列中的事件进行处理时，会调用相应的处理函数，这就实现了基于事件的回调。因为 Redis 一直在对事件队列进行处理，所以能及时响应客户端请求，提升Redis 的响应性能。

select就是用1024个bit来代表1024个FD，有就绪的就通知，然后用户进程需要进行遍历才知道哪些是就绪的FD。存在的问题：需要将整个fd_set从⽤户空间拷⻉到内核空间，select结束还要再次拷⻉回⽤户空间；select⽆法得知具体是哪个fd就绪，需要遍历整个fd_set；fd_set监听的fd数量不能超过1024

poll是在select上稍微改进了一下。过程：1.创建pollfd数组，向其中添加关注的fd信息；2.调⽤poll函数，将pollfd数组拷⻉到内核空间，转链表存储；3.内核遍历fd，判断是否就绪；4.数据就绪或超时后，拷⻉pollfd数组到⽤户空间，返回就绪fd数量n；5.判断n是否⼤于0，⼤于0则遍历pollfd数组，找到就绪的fd。和select相比，无大小限制了，但是监听FD过多，遍历时间也更长。

epoll就是在内核空间，用一个红黑树来存FD（FD直接进入内核空间，无需多余拷贝），如果这个FD上面有相应的事件了，就放到list里。用户进程无需遍历所有的FD，就能知道哪些FD就绪了。

### 3. redis的持久化

#### AOF

##### 定义

记录日志，但是是执行写入操作后再记。（不会阻塞当前这个写操作，但是会影响紧接着的下一个写操作，还有个好处就是不用检查语法是否正确）

##### 写回的配置项

Always：同步写回

Everysec：每秒写回

No：由操作系统来控制什么时候写回

性能越来越好，丢失数据越来越多

##### 日志文件太大怎么办

有重写机制，一个key可能被设置多次，重写机制就是浓缩日志，有一条最新的设置key的语句就行。这个机制是重新启了一个新线程，复制了一份当时的内存数据来进行操作的。不会阻塞主线程。

#### RDB

##### 定义

记录某一时刻的数据，相比于AOF，能够更快恢复。

##### 是否阻塞？

redis提供了两个命令，分别是save和bgsave；

save：在主线程执行，会导致阻塞

bgsave：创建一个子进程，专门用来写入RDB文件，避免主线程阻塞。默认配置

##### 执行RDB时，如果有写入操作，会影响快照吗？

copy-on-write机制。首先bgsave子进程本身是由主线程fork生成的，共享主线程的所有内存数据。如果此时有个写入操作，这块数据会被复制一份，生成副本，bgsave会把副本数据写入RDB文件。而主线程仍然可以直接修改原来的数据。

##### 快照的间隔时间？

并不是间隔越小越好，首先fork子进程这个操作本身就是阻塞主线程的。然后假如数据很多，但是间隔时间短，很可能上一次还没结束，下一次就开始。

针对这个问题，我们可以做增量快照。做了一次全量快照后，后续的快照只对修改的数据进行快照记录，这样可以避免每次全量快照的开销。但是也不完美，因为会引入额外的空间来记录哪些是修改过的数据。

#### 最佳实践：

- RDB+AOF：数据不能丢失时，内存快照和 AOF 的混合使用是一个很好的选择
- RDB：如果允许分钟级别的数据丢失
- AOF：优先使用 everysec 的配置选项，因为它在可靠性和性能之间取了一个平衡。

## MongoDB

### 架构篇

![](/images/mongodb架构.png)

#### 1. 简单描述下mongodb的架构

**1. 分层结构**

- **单节点**：最基础的单机版，适合小项目。
- **副本集**：多个节点组成（主节点负责写，从节点备份），主挂了自动选新主，保证高可用。
- **分片集群**：数据太大时，把数据切碎分到多个节点（分片），像“分蛋糕”一样分散压力，解决容量和性能瓶颈。

**2. 数据模型**

- 存的是**文档**（BSON），像JSON一样灵活，字段可增可减，一条数据可以包含子文档或数组（比如订单里直接嵌入商品列表）。

**3. 存储引擎**

- 默认用**WiredTiger**，高效能引擎，支持压缩和并发操作，读写快还省空间。

#### 2. WiredTiger的优势

1. **存储方式**：WiredTiger使用B+树，MMAPv1使用线性存储
2. **锁粒度**：WiredTiger是文档级锁，并发写入性能更高，MMAPv1是集合级锁
3. **数据压缩与存储效率**：WiredTiger支持Snappy/Zlib压缩，存储占用降低70%+，且B+树结构减少磁盘碎片。MMAPv1无压缩，内存映射文件易产生碎片，空间利用率低。
4. **事务与MVCC支持**：WiredTiger：多版本并发控制（MVCC）+ 快照隔离，支持多文档ACID事务。MMAPv1：无事务支持，仅保证单文档原子性。

#### 3. WiredTiger引擎包含的文件和作用

- WiredTiger.basecfg：存储基本配置信息，与 ConfigServer有关系
- WiredTiger.lock：定义锁操作
- WiredTiger.turtle：存储WiredTiger.wt的元数据
- WiredTiger.wt：存储table*的元数据
- table*.wt：存储各张表的数据
- journal：存储WAL(Write Ahead Log)，wiredTiger也是日志先行的概念

#### 4. WiredTiger数据落盘

WiredTiger通过 **WAL（Write-Ahead Logging）机制** 和 **后台异步刷盘** 结合，实现高效且可靠的数据持久化：

**步骤1：写入WAL日志**

- 流程：
  1. 写操作先写入到Journal Buffer，然后每100ms从buffer写入到Journal文件
- 作用：
  - 确保数据持久性（即使数据库崩溃，可通过日志恢复）。
  - 允许先快速响应客户端，再异步刷盘。

**步骤2：更新内存数据**

- 内存操作：
  - 数据修改先通过copy on write的方式更新内存中的 **B+树节点**（或LSM树结构）。

**步骤3：后台异步刷盘**

- 触发条件：
  - 每60s或容量达到2GB后提交一次
  - 定期检查点（Checkpoint）或系统空闲时。
- 操作：
  1. 将内存中的数据（B+树节点）**批量写入数据文件**（如`collection-*.wt`）。
  2. 同步更新元数据（如`WiredTiger.metadata`）。

#### 5. WiredTiger的检查点Checkpoint

- 作用：
  - 定期生成 **数据文件与WAL日志的一致性快照**，标记当前内存数据已持久化的状态。
  - 减少崩溃恢复时需要重放的日志量。
- 流程：
  1. 将内存中所有修改的数据节点刷入数据文件。b+tree叶子节点存的都是page，所以刷的也是page
  2. 记录检查点位置到元数据文件（`WiredTiger.checkpoint`）。
  3. 清理过期的日志文件（仅保留从检查点开始的日志）。

### 索引篇

#### 1. mongodb中有哪些索引

```mongodb
db.goods.insertMany( [
    { item: "canvas", qty: 100, size: { h: 28, w: 35.5, uom: "cm" }, status:
    "A" },
    { item: "journal", qty: 25, size: { h: 14, w: 21, uom: "cm" }, status:
    "A" }
]);

db.inventory.insertMany([
    { _id: 1,item: "abc",stock: [{ size: "S", color: "red", quantity: 25 },{
    size: "S", color: "blue", quantity: 10 },{ size: "M", color: "blue",
    quantity: 50 }]},
    {_id:2,item:"def",stock:[{size:"S",color:"blue",quantity:20},
    {size:"M",color:"blue",quantity:5},{size:"M",color:"black",quantity:10},
    {size:"L",color:"red",quantity:2}]},
    {_id:3,item:"ijk",stock:[{size:"M",color:"blue",quantity:15},
    {size:"L",color:"blue",quantity:100},{size:"L",color:"red",quantity:25}]}
])
```

- 单键索引

  - db.集合名.createIndex({"字段名":排序方式}) 排序方式：1-升序 -1-降序
  - db.goods.createIndex( { "size.w": 1 } ) 嵌套子文档字段

- 复合索引

  - db.集合名.createIndex({"字段名1" : 排序方式, "字段名2" : 排序方式})

- 多键索引

  - 针对属性包含数组数据的情况

  - ```mongodb
    db.col1.insertMany([
        { _id: 5, type: "food", item: "aaa", ratings: [ 5, 8, 9 ] },
        { _id: 6, type: "food", item: "bbb", ratings: [ 5, 9 ] },
        { _id: 7, type: "food", item: "ccc", ratings: [ 9, 5, 8 ] },
        { _id: 8, type: "food", item: "ddd", ratings: [ 9, 5 ] },
        { _id: 9, type: "food", item: "eee", ratings: [ 5, 9, 5 ] }
    ])
    
    db.col1.createIndex( { ratings: 1 } )
    db.col1.find( { ratings: [ 5, 9 ] } ).explain() # 执行计划
    # 会发现："isMultiKey" : true, 表明查询使用到了多键索引
    # "indexBounds" : {"ratings" : ["[5.0, 5.0]","[[ 5.0, 9.0 ], [ 5.0, 9.0 ]]"]}
    # 这就是mongodb中使用多建索引的强大之处，mongodb首先会去整个文档的数组中查找首字母是5的；
    # 然后找到了"[5.0, 5.0]","[[ 5.0, 9.0 ], [ 5.0, 9.0 ]” 这些数组，然后在找下一个是9
    的，最终过滤出想要的结果
    ```

  - db.inventory.createIndex({ "stock.size": 1, "stock.quantity": 1 }) 其中stock是数组

- 地理空间索引

  - ```mongodb
    db.company.insert(
    	{loc : { type: "Point", coordinates: [ 116.482451, 39.914176 ] },name:"来广营地铁站-叶青北园",category : "Parks"}
    )
    
    db.company.ensureIndex( { loc : "2dsphere" } )
    
    # 参数不是1或-1，为2dsphere 或者 2d。还可以建立组合索引。
    db.company.find({
        "loc" : {
            "$geoWithin" : {
            	"$center":[[116.482451,39.914176],0.05]
            }
        }
    })
    ```

- 全文索引

  - db.集合.createIndex({"字段": "text"})

- 哈希索引

  - db.集合.createIndex({"字段": "hashed"})