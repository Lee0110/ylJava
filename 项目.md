# 项目

## 在线客服

### 1. 项目背景及需求分析

公司目前已经电话客服，在线客服还是使用第三方公司的服务，无法满足一些定制化需求，公司决定自研。

需求如下：

- 用户群体分为两类：客服人员和用户，用户又来自于不同的渠道，包括web端、app端、微信h5等
- 聊天由用户主动发起，分配客服人员后便可进行聊天
- 客服可查询用户的所有聊天记录
- 不需要语音和视频聊天

### 2. 网络模式选型

- client-server：客户端之间的通讯依赖服务端的转发。服务端持有所有客户端的信息，方便监控，但是服务器的最大连接数是瓶颈
- peer2peer：去中心化网络，客户端之间可两两建立连接。可保证客户端通讯的私密性，并且规模可以无限扩大，无需转发对于语音视频也有优势，但是应用时常需要解决网络穿透问题。

对于客服系统而言，连接数取决于客服人员的处理能力，且需要保存聊天记录，监控各种状态，所以采用client-server模式。

### 3. 应用层协议的选项

- HTTP轮询：客户端通过HTTP请求轮询服务端有没有新消息。如果没有新消息则马上返回，存在大量无用请求
- HTTP长轮询：有新消息才返回，否则超时等待，这样可以减少请求，但是每次请求都建立了连接，增加了服务端的网络I/O开销，对性能不友好
- WebSocket：全双工通信协议。在WebSocket API中，浏览器和服务器只需要完成一次握手，就可以创建持久性的可双向数据传输的连接。

WebSocket协议建立长连接和全双工通信的特点更适合客服系统

### 4. 在分布式架构下，用户和客服分别连接了不同的服务器，怎么收发消息？

问题描述：用户 A 连接到 Server 1。客服 B 连接到 Server 2。

现状：使用redis共享内存

最初系统采用redis作为全局路由表存储，主要存的是clientID -> serverID。每次客户端连接或断开时，都会更新这个路由表。消息发送时，通过查询Redis找到对应的目标服务器。

缺点：如果网络不稳定，客户端频繁建立、断开连接，可能出现：建立了连接1，断开了，此时由于网络问题还未删除全局路由表对应的数据，新连接建立了，然后删除请求又好了，redis根据key删除了连接。导致消息发送失败。

改进方案：使用广播策略

核心思想：

- 使用消息队列实现广播
- 每台服务器只保存自己连接的客户端信息，自己能处理就处理，不能就广播出去

优点：

不再依赖全局路由表，避免了网络抖动造成的问题。本地存储使用 Map<WsClientType, ConcurrentHashMap<String,Channel>>（消息类型（seat/web/app/wechat/en_wechat）：用户id或坐席id：channel）。在删除的时候使用boolean remove(Object key, Object value)方法。

即使某台服务器宕机，其他服务器也能正常处理自己的连接，不会导致全局不可用，整体可用性提升

除此之外，我还将广播的各种方法抽象成接口，给了两套实现，一套kafka，一套redis。通过配置来判断走kafka还是redis。如果kafka或者redis某一个突然不可用，可迅速切换到另一个实现。

### 5. 消费者的幂等处理

kafka生产者发生时参数key传了一个UUID。消费者本地有一个缓存，根据key判断是否消费过

### 6. 坐席侧特殊处理

存在的问题：

- 消息处理存在阻塞操作，例如写入数据库，rpc调用机器人服务
- netty中channel的handler处理是由分配好的线程处理，而不是处理时再去现分配。一个线程可处理多个channel
- 坐席数量少但是重要，一个坐席可能同时服务多个用户，如果其handler和用户的handler由同一个线程处理，用户rpc调用阻塞了，会导致该坐席服务的所有用户都无法及时响应

解决办法：

- 区别对待用户端和坐席端
  - 用户端：继续使用Netty的默认线程模型
  - 坐席端：引入自定义线程池，具体实现就是在坐席的handler里的channelRead方法，直接交给线程池处理
  - 调用方法ctx.channel().pipeline().addLast()时，区分坐席和用户，如果是坐席就多传一个参数，自定义的线程池传进去
- 该方法的可行性：netty中pipeline的handler是串行执行的，保证这个业务handler是最后一个handler，才可以异步执行

效果：

这样改进后，坐席侧的延迟降低了，不会出现卡住的情况。